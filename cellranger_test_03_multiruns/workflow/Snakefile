# Snakefile of the workflow of scRNAseq_Stanard_COSR_pipeline

# ======================================================================
# == import libraries ==
# ======================================================================

from snakemake.utils import min_version
# min_version("8.25.5")
import pandas as pd
import os

# ======================================================================
# == import config ==
# ======================================================================

# load the config here because the steps below need some variables available in the config
configfile: "config/config.yaml"
# this is needed because the resources for RAM and cpus per rule are defined in the profile
configfile: "profiles/default/config.v8+.yaml"

# ======================================================================
# == load sample information ==
# ======================================================================

# read in the sample information: load the csv generated by pypette
df_sample = pd.read_csv(config["samples_csv"])

# --- implementation for single runs ---

# 1. Sort the DataFrame by 'sample_name' and 'sample_run'
df_sample2 = df_sample.sort_values(by=['sample_name', 'sample_run']).reset_index(drop=True)

# 2. Create a cumulative count within each 'sample_name' group: cumcount() starts at 0, so add 1 to start numbering from 1
df_sample2['sample_instance_num'] = df_sample2.groupby('sample_name').cumcount() + 1

# 3. Create the new unique sample name string
df_sample2['sample_name_unique'] = df_sample2['sample_name'] + '_' + df_sample2['sample_instance_num'].astype(str)

# generate a pandas dataframe. Here it is expected only one path per sample/run.
aggregated_single_alt = df_sample2.groupby(['sample_name','sample_run']).agg(
    sample_run=('sample_run', 'unique'),  # Get unique 'sample_run' per group
    sample_path_string=('sample_path', lambda s: sorted(s.map(os.path.dirname).unique())),
    # needed to check for the existance of the actual input files
    sample_path_list=('sample_path','unique'),
    # needed to avoid issues with the *.mro files during processing
    sample_name_unique=('sample_name_unique', 'first')
)

# 3. Convert the aggregated DataFrame to the desired dictionary format
SAMPLES_single = aggregated_single_alt.to_dict('index')

# --- implementation for merging runs ---

# generate a pandas dataframe. Here is expected a comma separated string of paths per sample.
aggregated_merge_alt = df_sample.groupby('sample_name').agg(
    # sample_run=('sample_run', lambda s: ",".join(s.unique())),
    # Aggregate sample_paths:
    # 1. Apply os.path.dirname to each path in the group (s.map(os.path.dirname))
    # 2. Find the unique directory names (.unique())
    # 3. Sort the unique directory names (sorted(...))
    # 4. Join the sorted unique directory names with a comma (",".join(...))
    sample_path_string=('sample_path', lambda s: ",".join(sorted(s.map(os.path.dirname).unique()))),
    # needed to check for the existance of the actual input files
    sample_path_list=('sample_path','unique')
)

# 3. Convert the aggregated DataFrame to the desired dictionary format
SAMPLES_merge = aggregated_merge_alt.to_dict('index')

# ======================================================================
# == load rules ==
# ======================================================================

# load the rules here because the variable SAMPLES needs to be available
# include: "rules/helper_function.smk"
include: "rules/cellranger_process.smk"
include: "rules/cellranger_multiRun.smk"
include: "rules/cellranger_multiRun_IntronExon.smk"

# ======================================================================
# == pipeline main rules (calling for the outputs) ==
# ======================================================================

# --- single run implementation ---

# # minimal run of cellranger
# rule cellranger_singlerun_minimal:
#     '''
#     rule to run the minimal analysis using cellranger.
#     '''
#     input:
#         expand(rules.runCellranger.output.output,
#                 zip, # Specify zip mode
#                 sample_name=[k[0] for k in SAMPLES_single.keys()],
#                 sample_run=[k[1] for k in SAMPLES_single.keys()])

# default rule to run the cellranger pipeline
rule cellranger_singlerun_default:
    '''
    rule to run the default analysis using cellrange for singlerun projects.
    '''
    input:
        expand(rules.moveCellrangerSummary.output.summary,
                zip, # Specify zip mode
                # this needs to remain for the call of the sample name in cellranger
                sample_name=[k[0] for k in SAMPLES_single.keys()],
                sample_run=[k[1] for k in SAMPLES_single.keys()]),
        
        expand(rules.runMultiqc1.output.html)

        # comment above an uncomment below in case you want to run the multiqc on the single run
        # expand(rules.runMultiqc1.output.html,
        #         zip, # Specify zip mode
        #         sample_run=[k[1] for k in SAMPLES_single.keys()])

# --- multiple runs implementation ---

# # minimal run of cellranger
# rule cellranger_multirun_minimal:
#     '''
#     rule to run the minimal analysis using cellranger from multirun projects
#     '''
#     input:
#         expand(rules.runCellrangerMultiRun.output.output,
#                 sample_name=SAMPLES_merge.keys())

# default rule to run the cellranger pipeline
rule cellranger_multirun_default:
    '''
    rule to run the default analysis using cellrange for multirun projects.
    '''
    input:
        expand(rules.moveCellrangerSummaryMultiRun.output.summary,
                sample_name=SAMPLES_merge.keys()),

        expand(rules.runMultiqc1MultiRun.output.html)


# default rule to run cellranger with IntronExon split outputs
rule multirun_pure_IntronExon:
    '''
    rule to produce the cellranger multirun default output and the pure intron exon results.
    '''
    input:
        expand(rules.moveCellrangerSummaryMultiRun.output.summary,
                sample_name=SAMPLES_merge.keys()),
        expand(rules.moveCellrangerIntronExonMultiRun.output.bam_file,
                sample_name=SAMPLES_merge.keys()),
        rules.copyGtfMultiRun.output.gtf,

        expand(rules.pureExonMultiRun.output.exon_count,
                sample_name=SAMPLES_merge.keys()),
        expand(rules.pureIntronMultiRun.output.intron_count,
                sample_name=SAMPLES_merge.keys()),
        expand(rules.pureAllMultiRun.output.all_count,
                sample_name=SAMPLES_merge.keys()),

        expand(rules.runMultiqc1MultiRun.output.html)
        
        
